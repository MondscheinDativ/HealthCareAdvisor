name: Knowledge Graph Weekly Update

# 添加权限设置，允许工作流写入仓库内容
permissions:
  contents: write

on:
  schedule:
    - cron: '0 0 * * 0'  # 每周日UTC午夜运行 (北京时间周一8:00)
  workflow_dispatch:  # 允许手动触发

jobs:
  crawl:
    runs-on: ubuntu-latest
    timeout-minutes: 120  # 增加超时时间到2小时
    defaults:
      run:
        working-directory: .  # 在根目录执行

    steps:
      - name: Checkout repository
        uses: actions/checkout@v4
        with:
          # 添加fetch-depth参数，获取完整的仓库历史
          fetch-depth: 0
          
      - name: Set up Python 3.10
        uses: actions/setup-python@v5
        with:
          python-version: '3.10'
          
      - name: Install dependencies
        run: |
          pip install --upgrade pip
          pip install -r knowledge_graph/requirements.txt
          
      - name: Show current directory
        run: pwd
      
      - name: List directory contents
        run: ls -la
      
      - name: Create raw data directory if not exists
        run: mkdir -p knowledge_graph/data/raw
      
      - name: Run ClinicalTrials.gov crawler
        run: |
          echo "Starting ClinicalTrials.gov crawler..."
          python knowledge_graph/crawlers/clinical_trials_gov.py > clinical_trials.log 2>&1
          echo "Crawler finished. Checking for output files..."
          if [ -d "knowledge_graph/data/raw" ]; then
            ls -la knowledge_graph/data/raw
          else
            echo "Directory knowledge_graph/data/raw does not exist!"
          fi
        continue-on-error: true  # 即使失败也继续
        
      - name: Run PubMed crawler
        run: |
          echo "Starting PubMed crawler..."
          python knowledge_graph/crawlers/pubmed_crawler.py > pubmed.log 2>&1
          echo "Crawler finished. Checking for output files..."
          if [ -d "knowledge_graph/data/raw" ]; then
            ls -la knowledge_graph/data/raw
          else
            echo "Directory knowledge_graph/data/raw does not exist!"
          fi
        continue-on-error: true  # 即使失败也继续
        
      - name: Run NIH DSLD crawler
        run: |
          echo "Starting NIH DSLD crawler..."
          python knowledge_graph/crawlers/nih_dsld.py > nih_dsld.log 2>&1
          echo "Crawler finished. Checking for output files..."
          if [ -d "knowledge_graph/data/raw" ]; then
            ls -la knowledge_graph/data/raw
          else
            echo "Directory knowledge_graph/data/raw does not exist!"
          fi
        continue-on-error: true  # 即使失败也继续
        
      - name: Upload log files
        uses: actions/upload-artifact@v4
        if: always()  # 即使失败也上传
        with:
          name: crawl-logs
          path: |
            clinical_trials.log
            pubmed.log
            nih_dsld.log
            
      - name: Upload generated data files
        uses: actions/upload-artifact@v4
        if: always()
        with:
          name: generated-data
          path: knowledge_graph/data/raw/
          retention-days: 7  # 保留7天
      
      - name: Commit and push changes
        run: |
          # 检查是否有数据文件生成
          if [ -d "knowledge_graph/data/raw" ] && [ -n "$(find knowledge_graph/data/raw -type f)" ]; then
            echo "Found data files. Committing and pushing changes..."
            git config --global user.name 'GitHub Actions Bot'
            git config --global user.email '41898282+github-actions[bot]@users.noreply.github.com'
            git status
            git add knowledge_graph/data/raw/*
            # 检查是否有文件需要提交
            if ! git diff --cached --quiet; then
              git commit -m "Auto-update: KG data refresh $(date +'%Y-%m-%d')"
              # 使用GITHUB_TOKEN环境变量推送
              git push "https://${{ github.actor }}:${{ secrets.GITHUB_TOKEN }}@github.com/${{ github.repository }}" HEAD:${{ github.ref }}
            else
              echo "No changes to commit"
            fi
          else
            echo "No data files to commit"
            exit 1  # 让这个步骤失败，便于调试
          fi
        continue-on-error: false  # 失败时不继续    
